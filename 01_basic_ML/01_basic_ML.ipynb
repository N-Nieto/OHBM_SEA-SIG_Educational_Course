{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9421706",
   "metadata": {},
   "source": [
    "<a   href=\"https://colab.research.google.com/github/N-Nieto/OHBM_SEA-SIG_Educational_Course/blob/master/01_basic_ML/01_basics_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51efd8",
   "metadata": {},
   "source": [
    "### If you are running in Google Colab, uncomment the cell below to load the data.\n",
    "### If you are running locally, ignore the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from urllib.request import urlretrieve\n",
    "# # Clean files\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Download necessary data files\n",
    "# Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "# # 01_basic_ML.ipynb needs this files\n",
    "# urlretrieve('https://zenodo.org/records/17056022/files/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv?download=1', './data/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv')\n",
    "# urlretrieve('https://zenodo.org/records/17056022/files/cleaned_IXI_behavioural.csv?download=1', './data/cleaned_IXI_behavioural.csv')\n",
    "\n",
    "# # 02_XAI.ipynb needs also this files\n",
    "# urlretrieve('https://zenodo.org/records/17056022/files/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv?download=1', './data/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv')\n",
    "\n",
    "# # Load data\n",
    "# df_behav = pd.read_csv(\"data/cleaned_IXI_behavioural.csv\", index_col=0)\n",
    "\n",
    "# # Some height values are not sensible, we filter them out\n",
    "# height = df_behav[\"HEIGHT\"].values\n",
    "# df_behav = df_behav[np.logical_and(height > 120, height < 200)]\n",
    "\n",
    "# # Remove NaNs and duplicates\n",
    "# df_behav.dropna(inplace=True)\n",
    "# df_behav.drop_duplicates(keep='first', inplace=True)\n",
    "# df_behav.to_csv('data/cleaned_IXI_behavioural.csv')\n",
    "\n",
    "# # Remove NaNs\n",
    "# df_cortical_100 = pd.read_csv(\"data/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv\", index_col=0)\n",
    "# df_cortical_100.dropna(inplace=True)\n",
    "# df_cortical_100.to_csv('data/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv')\n",
    "\n",
    "# # Remove NaNs\n",
    "# df_subcortical = pd.read_csv(\"data/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv\", index_col=0)\n",
    "# df_subcortical.dropna(inplace=True)\n",
    "# df_subcortical.to_csv('data/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv')\n",
    "\n",
    "# data_path = Path(\"data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f783a4",
   "metadata": {},
   "source": [
    "## Basic Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdaea3",
   "metadata": {},
   "source": [
    "##  1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# libraries for data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "if 'data_path' not in locals():\n",
    "    data_path = Path(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the behavioural data\n",
    "df_behav = pd.read_csv(data_path / \"cleaned_IXI_behavioural.csv\", index_col=0)\n",
    "\n",
    "# get the imaging data: we will use gray matter volume (GMV) \n",
    "# of 100 cortical parcels from the Schaefer 100x17 parcellation\n",
    "df_GMV = pd.read_csv(data_path /  \"cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv\", index_col=0)\n",
    "columns_GMV = df_GMV.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine behavioural and GMV data\n",
    "# as the subject IDs are the index, we can use join\n",
    "df_data = df_behav.join(df_GMV, how=\"inner\")\n",
    "print(df_data.head())\n",
    "\n",
    "# histogram the height (our target variable) as a sanity check\n",
    "plt.hist(df_data[\"HEIGHT\"], bins=30)\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Height\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa823e4",
   "metadata": {},
   "source": [
    "## 2. First prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model to predict height of a person using their neuroimaging data. \n",
    "# features (X): gray matter volume of different brain regions\n",
    "# target (y): height of a person\n",
    "# model (h): linear regression, i.e. ordinary least squares (OLS)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# do train-test split to evaluate the model on unseen data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# the function resturns dataframes for training and test data\n",
    "# 80% training, 20% test (passed as test_size=0.2)\n",
    "# set random_state for reproducibility\n",
    "train_data, test_data = train_test_split(df_data, test_size=0.2, random_state=1)\n",
    "\n",
    "# train a OLS regression model\n",
    "model = LinearRegression()\n",
    "# fit the model using training data\n",
    "# we need both features (X) and target (y)\n",
    "# this is where the model learns the parameters, i.e. weights\n",
    "model.fit(X=train_data[columns_GMV], y=train_data[\"HEIGHT\"])\n",
    "\n",
    "# predict test data using the trained model\n",
    "# we only need features (X)\n",
    "# the model will predict the target (y)\n",
    "y_pred = model.predict(X=test_data[columns_GMV])\n",
    "\n",
    "# evaluate the model: we know the true target values\n",
    "y_true = test_data[\"HEIGHT\"].values\n",
    "# get correlation and r2_Score\n",
    "r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# also get the training performance\n",
    "y_train_pred = model.predict(X=train_data[columns_GMV])\n",
    "y_train_true = train_data[\"HEIGHT\"].values\n",
    "\n",
    "r_train = np.corrcoef(y_train_true, y_train_pred)[0, 1]\n",
    "r2_train = r2_score(y_train_true, y_train_pred)\n",
    "\n",
    "print(\"OLS performance (train-test split):\")\n",
    "print(f\"Train: r={r_train:.2f}, r2={r2_train:.2f}\")\n",
    "print(f\"Test : r={r:.2f}, r2={r2:.2f}\")\n",
    "\n",
    "# scatterplot true versus predicted test set values\n",
    "plt.figure(figsize=(3, 2))\n",
    "sns.regplot(x=y_true, y=y_pred)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"Test (OLS): r={r:.2f}, r2={r2:.2f}\")\n",
    "plt.show()\n",
    "\n",
    "# plot the training set performance in red\n",
    "plt.figure(figsize=(3, 2))\n",
    "sns.regplot(x=y_train_true, y=y_train_pred, color='red')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"Train (OLS): r={r_train:.2f}, r2={r2_train:.2f}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32557bb5",
   "metadata": {},
   "source": [
    "1. What is the relationship between the training and test performance?\n",
    "2. What does this tell us about the OLS model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add867f0",
   "metadata": {},
   "source": [
    "## 3. Ridge regression: a regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57eda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we improve with L2 regularization (Ridge regression)? \n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge()\n",
    "# we already have train-test split from before\n",
    "model.fit(X=train_data[columns_GMV], y=train_data[\"HEIGHT\"])\n",
    "y_pred = model.predict(X=test_data[columns_GMV])\n",
    "\n",
    "r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# also check training performance\n",
    "y_train_pred = model.predict(X=train_data[columns_GMV])\n",
    "r_train = np.corrcoef(y_train_true, y_train_pred)[0, 1]\n",
    "r2_train = r2_score(y_train_true, y_train_pred)\n",
    "\n",
    "print(\"Ridge performance (train-test split):\")\n",
    "print(f\"Train: r={r_train:.2f}, r2={r2_train:.2f}\")\n",
    "print(f\"Test : r={r:.2f}, r2={r2:.2f}\")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(3, 2))\n",
    "sns.regplot(x=y_true, y=y_pred)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"Test (Ridge): r={r:.2f}, r2={r2:.2f}\")\n",
    "plt.show()\n",
    "\n",
    "# plot the training set performance\n",
    "plt.figure(figsize=(3, 2))\n",
    "sns.regplot(x=y_train_true, y=y_train_pred, color='red')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"Train (Ridge): r={r_train:.2f}, r2={r2_train:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370ac70",
   "metadata": {},
   "source": [
    "1. Why did the performance improve compared to OLS?\n",
    "2. Is the test performance always lower than the train performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f17c05",
   "metadata": {},
   "source": [
    "## 4. k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c759ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation: more efficient use of data\n",
    "# we will use 5-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set up\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Store results\n",
    "test_r_scores = []\n",
    "test_r2_scores = []\n",
    "train_r_scores = []\n",
    "train_r2_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(outer_cv.split(df_data)):\n",
    "    # calling split on the KFold object returns \n",
    "    # indices for train and test sets\n",
    "    # use iloc to get the actual dataframes\n",
    "    train_data = df_data.iloc[train_index]\n",
    "    test_data = df_data.iloc[test_index]\n",
    "\n",
    "    # Train Ridge model\n",
    "    model = Ridge()\n",
    "    model.fit(train_data[columns_GMV], train_data[\"HEIGHT\"])\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(test_data[columns_GMV])\n",
    "    y_true = test_data[\"HEIGHT\"].values\n",
    "\n",
    "    # Evaluate\n",
    "    test_r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    test_r2 = r2_score(y_true, y_pred)\n",
    "    test_r_scores.append(test_r)\n",
    "    test_r2_scores.append(test_r2)\n",
    "\n",
    "    print(f\"Fold {fold + 1}: r = {test_r:.3f}, r2 = {test_r2:.3f}\")\n",
    "    # plot fold-wise performance\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    sns.regplot(x=y_true, y=y_pred)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"Fold {fold + 1} (Ridge): r={test_r:.2f}, r2={test_r2:.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on the training data as well (do we overfit?)\n",
    "    y_train_pred = model.predict(train_data[columns_GMV])\n",
    "    y_train_true = train_data[\"HEIGHT\"].values\n",
    "    train_r = np.corrcoef(y_train_true, y_train_pred)[0, 1]\n",
    "    train_r2 = r2_score(y_train_true, y_train_pred)\n",
    "    train_r_scores.append(train_r)\n",
    "    train_r2_scores.append(train_r2)\n",
    "\n",
    "# Print performance\n",
    "print('Ridge performance (k-fold CV):')\n",
    "print(f\"Train: r = {np.mean(train_r_scores):.3f}, r2 = {np.mean(train_r2_scores):.3f}\")\n",
    "print(f\"Test : r = {np.mean(test_r_scores):.3f}, r2 = {np.mean(test_r2_scores):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2316b",
   "metadata": {},
   "source": [
    " Oh but we have one hyperparameter for Ridge:\n",
    " \n",
    "`alpha{float, ndarray of shape (n_targets,)}, default=1.0\n",
    "    Constant that multiplies the L2 term, controlling regularization strength. alpha must be a non-negative float i.e. in [0, inf).`\n",
    "\n",
    "How to choose it?\n",
    "Nested cross-validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127177df",
   "metadata": {},
   "source": [
    "## 5. Nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define alpha values to tests\n",
    "alphas = [0.01, 0.1, 0.5, 1.0, 10.0]\n",
    "print(\"Optimizing alpha values using nested CV\")\n",
    "\n",
    "# Set up\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Store results\n",
    "test_r_scores = []\n",
    "test_r2_scores = []\n",
    "train_r_scores = []\n",
    "train_r2_scores = []\n",
    "best_alphas = []\n",
    "\n",
    "# outer CV loop\n",
    "# here we will do \"model evaluation\"\n",
    "for fold, (train_index, test_index) in enumerate(outer_cv.split(df_data)):\n",
    "    train_data = df_data.iloc[train_index]\n",
    "    test_data = df_data.iloc[test_index]\n",
    "\n",
    "    # Train Ridge model while optimizing alpha in a for loop\n",
    "    inner_score_max = -np.inf\n",
    "    best_alpha = None\n",
    "    for alpha in alphas:\n",
    "        # new for loop to test for each alpha\n",
    "        # we will do an inner cross-validation to find the best alpha\n",
    "        # for each alpha, we will do a 5-fold cross-validation on the training data\n",
    "        inner_cv = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "        inner_scores = []\n",
    "        # inner CV loop\n",
    "        # here we will do \"model selection\"\n",
    "        for train_index_inner, test_index_inner in inner_cv.split(train_data):\n",
    "            train_data_inner = train_data.iloc[train_index_inner]\n",
    "            test_data_inner = train_data.iloc[test_index_inner]\n",
    "            model = Ridge(alpha=alpha)\n",
    "            model.fit(train_data_inner[columns_GMV], train_data_inner[\"HEIGHT\"])\n",
    "            inner_scores.append(\n",
    "                model.score(test_data_inner[columns_GMV], test_data_inner[\"HEIGHT\"])\n",
    "            )\n",
    "        mean_inner_score = np.mean(inner_scores)\n",
    "        # check if this is the best alpha so far\n",
    "        if mean_inner_score > inner_score_max:\n",
    "            inner_score_max = mean_inner_score\n",
    "            best_alpha = alpha\n",
    "\n",
    "    # fit model with best alpha found in inner loop\n",
    "    # use the entire training data\n",
    "    best_alphas.append(best_alpha)\n",
    "    model = Ridge(alpha=best_alpha)\n",
    "    model.fit(train_data[columns_GMV], train_data[\"HEIGHT\"])\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(test_data[columns_GMV])\n",
    "    y_true = test_data[\"HEIGHT\"].values    \n",
    "\n",
    "    # Evaluate\n",
    "    test_r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    test_r2 = r2_score(y_true, y_pred)\n",
    "    test_r_scores.append(test_r)\n",
    "    test_r2_scores.append(test_r2)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold + 1}: r = {test_r:.3f}, r2 = {test_r2:.3f}, alpha = {best_alpha}\"\n",
    "    )\n",
    "\n",
    "    # Evaluate on the training data as well (do we overfit?)\n",
    "    y_train_pred = model.predict(train_data[columns_GMV])\n",
    "    y_train_true = train_data[\"HEIGHT\"].values\n",
    "    train_r = np.corrcoef(y_train_true, y_train_pred)[0, 1]\n",
    "    train_r2 = r2_score(y_train_true, y_train_pred)\n",
    "    train_r_scores.append(train_r)\n",
    "    train_r2_scores.append(train_r2)\n",
    "\n",
    "print('Ridge performance (k-fold CV):')\n",
    "print(f\"Train: r = {np.mean(train_r_scores):.3f}, r2 = {np.mean(train_r2_scores):.3f}\")\n",
    "print(f\"Test : r = {np.mean(test_r_scores):.3f}, r2 = {np.mean(test_r2_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75313185",
   "metadata": {},
   "source": [
    "1. What is the difference between train-test split and k-fold CV?\n",
    "2. What is the purpose of hyperparameter tuning?\n",
    "3. What is the difference between hyperparameters and parameters?\n",
    "4. What to do if the selected hyperparameter is at the edge of the search space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb30511",
   "metadata": {},
   "source": [
    "## 6. Using scikit-learn's GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d550381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the grid search for alpha values\n",
    "# notice here we define a dictionary\n",
    "# the key is the hyperparameter name of the model\n",
    "# the value is a list of values to test\n",
    "param_grid = {\"alpha\": [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "print(\"Optimizing alpha values using grid search\")\n",
    "\n",
    "# Set up\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Store results\n",
    "r_list = []\n",
    "r2_list = []\n",
    "best_alphas = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(outer_cv.split(df_data)):\n",
    "    train_data = df_data.iloc[train_index]\n",
    "    test_data = df_data.iloc[test_index]\n",
    "\n",
    "    # Find best alpha using GridSearchCV instead of for loop\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "    # define the model as a GridSearchCV object\n",
    "    model = GridSearchCV(\n",
    "        Ridge(), param_grid, cv=inner_cv, scoring=\"r2\"\n",
    "    )  # r2 uses r2_score\n",
    "    # fit will find the best alpha: it will do the inner cross-validation\n",
    "    model.fit(train_data[columns_GMV], train_data[\"HEIGHT\"])\n",
    "\n",
    "    # predicting using this model it will use \n",
    "    # the \"best\" alpha, let's look at that value\n",
    "    best_alpha = model.best_params_[\"alpha\"]\n",
    "    best_alphas.append(best_alpha)\n",
    "\n",
    "    # Predict using the best alpha\n",
    "    y_pred = model.predict(test_data[columns_GMV])\n",
    "    y_true = test_data[\"HEIGHT\"].values\n",
    "\n",
    "    # Evaluate\n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    r_list.append(r)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "    print(f\"Fold {fold + 1}: r = {r:.3f}, r2 = {r2:.3f}, alpha = {best_alpha}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dab49b",
   "metadata": {},
   "source": [
    "1. Are the results the same as in the previous section?\n",
    "2. Why/why not?\n",
    "3. What is the advantage of using GridSearchCV?\n",
    "4. There is a `cross_val_score` function in sklearn which will remove the outer loop. We do not use it here because we want to learn the concepts of hyperparameter tuning and model selection in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6adda5b",
   "metadata": {},
   "source": [
    "## 7. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's move to classification\n",
    "\n",
    "# first create discrete labels for height\n",
    "# using median height as cutoff\n",
    "# according to NHS mean height for men is 175 and for women is 162\n",
    "# let's take average of these two values as cutoff\n",
    "height_cutoff = (175 + 162) / 2\n",
    "print(f\"Height cutoff: {height_cutoff} cm\")\n",
    "\n",
    "# create a new column in df for tall (1) and short (0)\n",
    "df_data[\"TALL\"] = (df_data[\"HEIGHT\"] >= height_cutoff).astype(int)\n",
    "df_data[\"TALL\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d8df0",
   "metadata": {},
   "source": [
    "1. Do the classes look balanced? Why is this important? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification needs different models and metrics\n",
    "# we will use RidgeClassifier and accuracy and ROC AUC as metrics\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# define the grid search for alhpa values\n",
    "param_grid = {\"alpha\": [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "\n",
    "# Set up CV\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Store results\n",
    "test_accuracies = []\n",
    "test_roc_aucs = []\n",
    "train_accuracies = []\n",
    "train_roc_aucs = []\n",
    "best_alphas = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(outer_cv.split(df_data)):\n",
    "    train_data = df_data.iloc[train_index]\n",
    "    test_data = df_data.iloc[test_index]\n",
    "\n",
    "    # Train Ridge model: now using GridSearchCV instead of for loop\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "    model = GridSearchCV(\n",
    "        RidgeClassifier(), param_grid, cv=inner_cv, scoring=\"accuracy\"\n",
    "    )  # accuracy for classification\n",
    "    model.fit(train_data[columns_GMV], train_data[\"TALL\"])\n",
    "    # when we predict using this model it will use the \"best\" alpha value, let's save that value\n",
    "    best_alpha = model.best_params_[\"alpha\"]\n",
    "    best_alphas.append(best_alpha)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(test_data[columns_GMV])\n",
    "    y_true = test_data[\"TALL\"].values\n",
    "\n",
    "    # Evaluate\n",
    "    test_acc = accuracy_score(y_true, y_pred)\n",
    "    test_roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_roc_aucs.append(test_roc_auc)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold + 1}: acc = {test_acc:.3f}, ROC AUC = {test_roc_auc:.3f}, alpha = {best_alpha}\"\n",
    "    )\n",
    "\n",
    "    # Evaluate on the training data as well (do we overfit?)\n",
    "    y_train_pred = model.predict(train_data[columns_GMV])\n",
    "    y_train_true = train_data[\"TALL\"].values\n",
    "    train_acc = accuracy_score(y_train_true, y_train_pred)\n",
    "    train_roc_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_roc_aucs.append(train_roc_auc)\n",
    "\n",
    "\n",
    "# print mean accuracy\n",
    "print('Ridge classifier performance (k-fold CV):')\n",
    "print(\n",
    "    f\"Train: acc={np.mean(train_accuracies):.3f}, ROC AUC={np.mean(train_roc_aucs):.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test : acc={np.mean(test_accuracies):.3f}, ROC AUC={np.mean(test_roc_aucs):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60140351",
   "metadata": {},
   "source": [
    "## 8. Data preprocessing using a `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also scale the data before classification\n",
    "# use sklearn's pipeline for that \n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the pipeline\n",
    "# new code\n",
    "pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"ridge\", RidgeClassifier())])\n",
    "\n",
    "# define the grid search for alpha values\n",
    "# note the change in the key name to match the pipeline step\n",
    "param_grid = {\n",
    "    \"ridge__alpha\": [0.01, 0.1, 0.5, 1.0, 10.0]\n",
    "}\n",
    "print(\"Optimizing alpha values using grid search\")\n",
    "\n",
    "# Set up\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Store results\n",
    "test_accuracies = []\n",
    "test_roc_aucs = []\n",
    "train_accuracies = []\n",
    "train_roc_aucs = []\n",
    "best_alphas = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(outer_cv.split(df_data)):\n",
    "    train_data = df_data.iloc[train_index]\n",
    "    test_data = df_data.iloc[test_index]\n",
    "\n",
    "    # Train Ridge model: now using GridSearchCV instead of for loop\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "    # Use a pipeline to scale the data before classification\n",
    "    # scaling becomes part of the model\n",
    "    # scaling parameters are estimated from the training data only\n",
    "    # no data leakage!\n",
    "    model = GridSearchCV(\n",
    "        pipeline, param_grid, cv=inner_cv, scoring=\"accuracy\"\n",
    "    )  # accuracy for classification\n",
    "    model.fit(train_data[columns_GMV], train_data[\"TALL\"])\n",
    "    # when we predict using this model it will use the \"best\" alpha value, let's save that value\n",
    "    best_alpha = model.best_params_[\"ridge__alpha\"]\n",
    "    best_alphas.append(best_alpha)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(test_data[columns_GMV])\n",
    "    y_true = test_data[\"TALL\"].values\n",
    "\n",
    "    # Evaluate\n",
    "    test_acc = accuracy_score(y_true, y_pred)\n",
    "    test_roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_roc_aucs.append(test_roc_auc)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold + 1}: accuracy = {test_acc:.3f}, roc_auc = {test_roc_auc:.3f}, alpha = {best_alpha}\"\n",
    "    )\n",
    "\n",
    "    # Evaluate on the training data as well (do we overfit?)\n",
    "    y_train_pred = model.predict(train_data[columns_GMV])\n",
    "    y_train_true = train_data[\"TALL\"].values\n",
    "    train_acc = accuracy_score(y_train_true, y_train_pred)\n",
    "    train_roc_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_roc_aucs.append(train_roc_auc)\n",
    "\n",
    "\n",
    "# print mean accuracy\n",
    "print('Performance:')\n",
    "print(\n",
    "    f\"Train: acc={np.mean(train_accuracies):.3f}, ROC AUC={np.mean(train_roc_aucs):.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test : acc={np.mean(test_accuracies):.3f}, ROC AUC={np.mean(test_roc_aucs):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9cbf8",
   "metadata": {},
   "source": [
    "1. What will you use as key for `alpha` `param_grid` values if the classifier step in the pipeline is named `clf`?\n",
    "2. Which preprocessing steps would you add to the pipeline?\n",
    "3. Why is it important to include preprocessing steps in the pipeline when performing cross-validation?\n",
    "4. Can all classifiers give ROC AUC scores? Why/why not? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FZJ_collaborations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec43c5c",
   "metadata": {},
   "source": [
    "<a   href=\"https://colab.research.google.com/github/N-Nieto/OHBM_SEA-SIG_Educational_Course/blob/master/03_pitfalls/03_01_pitfals_confounding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ddf37",
   "metadata": {},
   "source": [
    "### If you are running in Google Colab, uncomment the cell below to load the data.\n",
    "### If you are running locally, ignore the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3194440",
   "metadata": {},
   "source": [
    "For questions on this notebook contact: v.komeyer@fz-juelich.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from urllib.request import urlretrieve\n",
    "# # Clean files\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Download necessary data files\n",
    "# Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "# # 01_basic_ML.ipynb needs this files\n",
    "# urlretrieve('https://zenodo.org/records/17056022/files/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv?download=1', './data/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv')\n",
    "# urlretrieve('https://zenodo.org/records/17056022/files/cleaned_IXI_behavioural.csv?download=1', './data/cleaned_IXI_behavioural.csv')\n",
    "\n",
    "# # 02_XAI.ipynb needs also this files\n",
    "# urlretrieve('https://zenodo.org/records/17056022/files/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv?download=1', './data/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv')\n",
    "\n",
    "# # Load data\n",
    "# df_behav = pd.read_csv(\"data/cleaned_IXI_behavioural.csv\", index_col=0)\n",
    "\n",
    "# # Some height values are not sensible, we filter them out\n",
    "# height = df_behav[\"HEIGHT\"].values\n",
    "# df_behav = df_behav[np.logical_and(height > 120, height < 200)]\n",
    "\n",
    "# # Remove NaNs and duplicates\n",
    "# df_behav.dropna(inplace=True)\n",
    "# df_behav.drop_duplicates(keep='first', inplace=True)\n",
    "# df_behav.to_csv('data/cleaned_IXI_behavioural.csv')\n",
    "\n",
    "# # Remove NaNs\n",
    "# df_cortical_100 = pd.read_csv(\"data/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv\", index_col=0)\n",
    "# df_cortical_100.dropna(inplace=True)\n",
    "# df_cortical_100.to_csv('data/cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv')\n",
    "\n",
    "# # Remove NaNs\n",
    "# df_subcortical = pd.read_csv(\"data/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv\", index_col=0)\n",
    "# df_subcortical.dropna(inplace=True)\n",
    "# df_subcortical.to_csv('data/cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv')\n",
    "\n",
    "# data_path = Path(\"data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8f87b",
   "metadata": {},
   "source": [
    "# Detect and Mitigate Machine Learning Pitfalls - Confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "if 'data_path' not in locals():\n",
    "    data_path = Path(\"../data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8c279",
   "metadata": {},
   "source": [
    "## Part 1 - Preparations and recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8877639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_cortical = pd.read_csv(data_path / 'cleaned_VBM_GM_Schaefer100x17_mean_aggregation.csv', index_col=0)\n",
    "df_subcortical = pd.read_csv(data_path / 'cleaned_VBM_GM_TianxS1x3TxMNI6thgeneration_mean_aggregation.csv', index_col=0)\n",
    "df_beh = pd.read_csv(data_path / 'cleaned_IXI_behavioural.csv', index_col=0)\n",
    "\n",
    "df_brain = df_cortical.join(df_subcortical, how=\"inner\")\n",
    "print(f\"Brain data shape: {df_brain.shape}\")\n",
    "\n",
    "df_data = df_beh.join(df_brain, how=\"inner\")\n",
    "print(f\"Final data shape: {df_data.shape}\")\n",
    "df_beh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs in confounding columns\n",
    "confounding_cols = [\"SEX_ID (1=m, 2=f)\", \"AGE\"]\n",
    "for col in confounding_cols:\n",
    "    if df_beh[col].isna().sum() > 0:\n",
    "        print(f\"{df_beh[col].isna().sum()} NaNs in confounding column {col}.\")\n",
    "        print(\"Drop NaNs and align subjects\")\n",
    "        df_beh = df_beh.dropna()\n",
    "        df_brain = df_brain.loc[df_beh.index]\n",
    "        print(f\"New behavioral data shape: {df_beh.shape}\")\n",
    "        print(f\"New brain data shape: {df_brain.shape}\")\n",
    "    else:\n",
    "        print(f\"No NaNs in confounding column {col}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the same ridge model as before but with full manually implemented nested CV\n",
    "GMV_columns = df_cortical.columns.to_list() + df_subcortical.columns.to_list()\n",
    "y = df_data['HEIGHT'].values\n",
    "X = df_data[GMV_columns].values  # only brain features\n",
    "\n",
    "# CV schemes\n",
    "n_outer = 5\n",
    "n_inner = 5\n",
    "outer_cv = KFold(n_splits=n_outer, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=n_inner, shuffle=True, random_state=42)\n",
    "\n",
    "# hyperparameter grid\n",
    "alphas = [0.001, 0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "# initialize lists to store results\n",
    "outer_results = []\n",
    "best_alphas_outer = []\n",
    "\n",
    "# loop over outer CV folds\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"Outer fold {outer_fold + 1}\")\n",
    "\n",
    "    X_outer_train, X_outer_test = X[outer_train_idx], X[outer_test_idx]\n",
    "    y_outer_train, y_outer_test = y[outer_train_idx], y[outer_test_idx]\n",
    "\n",
    "    # Inner CV scheme\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # --------- inner CV for hyperparameter tuning ---------\n",
    "    alpha_scores = {alpha: [] for alpha in alphas}\n",
    "\n",
    "    # iterate inner folds for hyperparameter evaluation\n",
    "    for inner_train_idx, inner_val_idx in inner_cv.split(X_outer_train, y_outer_train):\n",
    "        X_inner_train, X_inner_val = X_outer_train[inner_train_idx], X_outer_train[inner_val_idx]\n",
    "        y_inner_train, y_inner_val = y_outer_train[inner_train_idx], y_outer_train[inner_val_idx]\n",
    "\n",
    "        # Standardize features\n",
    "        scaler_inner = StandardScaler()\n",
    "        X_inner_train_scaled = scaler_inner.fit_transform(X_inner_train)\n",
    "        X_inner_val_scaled = scaler_inner.transform(X_inner_val)\n",
    "\n",
    "        # For each alpha, train Ridge on inner_train and evaluate on inner_val\n",
    "        for alpha in alphas:\n",
    "            model = Ridge(alpha=alpha)\n",
    "            model.fit(X_inner_train_scaled, y_inner_train)\n",
    "            y_val_pred = model.predict(X_inner_val_scaled)\n",
    "            R2_val = r2_score(y_inner_val, y_val_pred)  # you can also use other metrics\n",
    "            alpha_scores[alpha].append(R2_val)\n",
    "    \n",
    "    # Aggregate inner scores (mean across inner folds) - median can also be a suitable choice\n",
    "    mean_alpha_scores = {alpha: np.mean(alpha_scores[alpha]) for alpha in alphas}\n",
    "    # pick the alpha with the highest mean inner CV R2\n",
    "    best_alpha = max(sorted(alphas), key=lambda x: mean_alpha_scores[x])\n",
    "    best_alphas_outer.append(best_alpha)\n",
    "\n",
    "    # ---------- retrain on full outer training set with best alpha and evaluate on outer test set ----------\n",
    "    # Fit scaler on outer training set\n",
    "    scaler_outer = StandardScaler()\n",
    "    X_outer_train_scaled = scaler_outer.fit_transform(X_outer_train)\n",
    "    X_outer_test_scaled = scaler_outer.transform(X_outer_test)\n",
    "\n",
    "    # Train Ridge with best alpha on full outer training set\n",
    "    model_outer = Ridge(alpha=best_alpha).fit(X_outer_train_scaled, y_outer_train)\n",
    "\n",
    "    # Evaluate on outer test set\n",
    "    y_outer_test_pred = model_outer.predict(X_outer_test_scaled)\n",
    "    R2_outer_test = r2_score(y_outer_test, y_outer_test_pred)\n",
    "    r_test = np.corrcoef(y_outer_test, y_outer_test_pred)[0, 1]\n",
    "    print(f\"  Best alpha: {best_alpha}\")\n",
    "    print(f\"  Outer Test R²: {R2_outer_test:.3f}\")\n",
    "    print(f\"  Outer Test Pearson r: {r_test:.3f}\")\n",
    "\n",
    "    # Evaluate train performance on outer_train (to check for overfitting)\n",
    "    y_outer_train_pred = model_outer.predict(X_outer_train_scaled)\n",
    "    R2_outer_train = r2_score(y_outer_train, y_outer_train_pred)\n",
    "    r_train = np.corrcoef(y_outer_train, y_outer_train_pred)[0, 1]\n",
    "    print(f\"  Outer Train R²: {R2_outer_train:.3f}\")\n",
    "    print(f\"  Outer Train Pearson r: {r_train:.3f}\")    \n",
    "\n",
    "    # Store all relevant info for this outer fold\n",
    "    outer_results.append({\n",
    "        \"fold\": outer_fold,\n",
    "        \"best_alpha\": best_alpha,\n",
    "        \"R2_train\": R2_outer_train,\n",
    "        \"R2_test\": R2_outer_test,\n",
    "        \"r_train\": r_train,\n",
    "        \"r_test\": r_test,\n",
    "        \"model\": model_outer,\n",
    "        \"scaler\": scaler_outer,\n",
    "        \"X_train_idx\": outer_train_idx,\n",
    "        \"X_test_idx\": outer_test_idx\n",
    "    })\n",
    "\n",
    "    print(f\"----- Done with outer fold {outer_fold+1} ------\\n\")\n",
    "\n",
    "# Summary\n",
    "mean_test_r2 = np.mean([res['R2_test'] for res in outer_results])\n",
    "mean_test_r = np.mean([res['r_test'] for res in outer_results])\n",
    "mean_train_r2 = np.mean([res['R2_train'] for res in outer_results])\n",
    "mean_train_r = np.mean([res['r_train'] for res in outer_results])\n",
    "print(\n",
    "    f\"\\nNested CV summary over {n_outer} folds: \\n\"\n",
    "    f\"mean R2_test={mean_test_r2:.3f}, mean r_test={mean_test_r:.3f}, \\n\"\n",
    "    f\"mean R2_train={mean_train_r2:.3f}, mean r_train={mean_train_r:.3f}\",\n",
    "    )\n",
    "print(\"Best alphas per outer fold:\", best_alphas_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train and test outer CV performance visually\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract values from results\n",
    "folds = [res['fold']+1 for res in outer_results]  # fold indices (1-based for nicer labeling)\n",
    "r2_train = [res['R2_train'] for res in outer_results]\n",
    "r2_test = [res['R2_test'] for res in outer_results]\n",
    "\n",
    "x = np.arange(len(folds))  # the label locations\n",
    "width = 0.35  # width of bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "bars1 = ax.bar(x - width/2, r2_train, width, label=\"Train R²\", color=\"skyblue\")\n",
    "bars2 = ax.bar(x + width/2, r2_test, width, label=\"Test R²\", color=\"salmon\")\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_xlabel(\"Outer Fold\")\n",
    "ax.set_ylabel(\"R² Score\")\n",
    "ax.set_title(\"Train vs Test R² per Outer Fold (Nested CV) - no confound removal\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(folds)\n",
    "ax.legend()\n",
    "\n",
    "# Add numbers on top of bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f\"{height:.2f}\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2aacad",
   "metadata": {},
   "source": [
    "## Part 2 - Adjusting for confounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b5eb7",
   "metadata": {},
   "source": [
    "From the DAG we know that one possible minimal set of variables to adjust for would be genetic factors, hormonal factors, maternal health/nutrition, childhood nutrition, childhood health/healthcare, sex and age. However, most of these variables are unmeasured, in fact of this list we only have sex and age measured. While it is still good to adjust for them, we have to keep in mind that we will remain with a partially biased model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same nested CV but now adjusting for confounding by linear feature residualization\n",
    "\n",
    "confounds = df_data[['SEX_ID (1=m, 2=f)','AGE']].values  # ADDTIONAL STEP\n",
    "\n",
    "# CV schemes\n",
    "n_outer = 5\n",
    "n_inner = 5\n",
    "outer_cv = KFold(n_splits=n_outer, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=n_inner, shuffle=True, random_state=42)\n",
    "\n",
    "# hyperparameter grid\n",
    "alphas = [0.001, 0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "# initialize lists to store results\n",
    "outer_results_with_cnfdrm = []\n",
    "best_alphas_outer_with_cnfdrm = []\n",
    "\n",
    "# loop over outer CV folds\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"Outer fold {outer_fold + 1}\")\n",
    "\n",
    "    X_outer_train, X_outer_test = X[outer_train_idx], X[outer_test_idx]\n",
    "    y_outer_train, y_outer_test = y[outer_train_idx], y[outer_test_idx]\n",
    "    # ADDITIONAL STEP - split the cnfounding variables\n",
    "    conf_outer_train, conf_outer_test = confounds[outer_train_idx], confounds[outer_test_idx]\n",
    "    # Sanity check prints\n",
    "    # print(\n",
    "    #     f\"   Length X_outer_train: {len(X_outer_train)}, \"\n",
    "    #     f\"   y_outer_train: {len(y_outer_train)}, \"\n",
    "    #     f\"   conf_outer_train: {len(conf_outer_train)}.\")\n",
    "    # print(\n",
    "    #     f\"   Length X_outer_test: {len(X_outer_test)}, \"\n",
    "    #     f\"   y_outer_test: {len(y_outer_test)}, \"\n",
    "    #     f\"   conf_outer_test: {len(conf_outer_test)}. \\n\")\n",
    "\n",
    "    # Inner CV scheme\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # --------- inner CV for hyperparameter tuning ---------\n",
    "    alpha_scores = {alpha: [] for alpha in alphas}\n",
    "\n",
    "    # iterate inner folds for hyperparameter evaluation\n",
    "    for inner_train_idx, inner_val_idx in inner_cv.split(X_outer_train, y_outer_train):\n",
    "        X_inner_train, X_inner_val = X_outer_train[inner_train_idx], X_outer_train[inner_val_idx]\n",
    "        y_inner_train, y_inner_val = y_outer_train[inner_train_idx], y_outer_train[inner_val_idx]\n",
    "        # ADDITIONAL STEP - split confounding variables\n",
    "        conf_inner_train, conf_inner_val = conf_outer_train[inner_train_idx], conf_outer_train[inner_val_idx]\n",
    "        # Sanity check prints\n",
    "        # print(\n",
    "        #     f\"   Length X_inner_train: {len(X_inner_train)}, \"\n",
    "        #     f\"   y_inner_train: {len(y_inner_train)}, \"\n",
    "        #     f\"   conf_inner_train: {len(conf_inner_train)}.\")\n",
    "        # print(\n",
    "        #     f\"   Length X_inner_val: {len(X_inner_val)}, \"\n",
    "        #     f\"   y_inner_val: {len(y_inner_val)}, \"\n",
    "        #     f\"   conf_inner_val: {len(conf_inner_val)}. \\n\")\n",
    "\n",
    "        # ADDITIONAL STEP - Fit confound model on inner_train (multi-output linear regression)\n",
    "        conf_model = LinearRegression().fit(conf_inner_train, X_inner_train)  # predict features from confounds\n",
    "\n",
    "        # ADDITIONAL STEP - Residualize features: subtract confound predictions\n",
    "        X_inner_train_resid = X_inner_train - conf_model.predict(conf_inner_train)\n",
    "        X_inner_val_resid = X_inner_val - conf_model.predict(conf_inner_val)\n",
    "\n",
    "        # Standardize RESIDUALIZED features\n",
    "        scaler_inner = StandardScaler()\n",
    "        X_inner_train_res_scaled = scaler_inner.fit_transform(X_inner_train_resid)\n",
    "        X_inner_val_res_scaled = scaler_inner.transform(X_inner_val_resid)\n",
    "\n",
    "        # For each alpha, train Ridge on inner_train and evaluate on inner_val\n",
    "        for alpha in alphas:\n",
    "            model = Ridge(alpha=alpha)\n",
    "            model.fit(X_inner_train_res_scaled, y_inner_train)\n",
    "            y_val_pred = model.predict(X_inner_val_res_scaled)\n",
    "            R2_val = r2_score(y_inner_val, y_val_pred)  # you can also use other metrics\n",
    "            alpha_scores[alpha].append(R2_val)\n",
    "    \n",
    "    # Aggregate inner scores (mean across inner folds) - median can also be a suitable choice\n",
    "    mean_alpha_scores = {alpha: np.mean(alpha_scores[alpha]) for alpha in alphas}\n",
    "    # pick the alpha with the highest mean inner CV R2\n",
    "    best_alpha = max(sorted(alphas), key=lambda x: mean_alpha_scores[x])\n",
    "    best_alphas_outer_with_cnfdrm.append(best_alpha)\n",
    "\n",
    "    # ---------- retrain on full outer training set with best alpha and evaluate on outer test set ----------\n",
    "    # ADDITIONAL STEP - Fit confound model on entire outer_train\n",
    "    conf_model_outer = LinearRegression().fit(conf_outer_train, X_outer_train)\n",
    "    X_outer_train_resid = X_outer_train - conf_model_outer.predict(conf_outer_train)\n",
    "    X_outer_test_resid = X_outer_test - conf_model_outer.predict(conf_outer_test)\n",
    "\n",
    "    # Fit scaler on outer training set (on RESIDUALIZED features)\n",
    "    scaler_outer = StandardScaler()\n",
    "    X_outer_train_res_scaled = scaler_outer.fit_transform(X_outer_train_resid)\n",
    "    X_outer_test_res_scaled = scaler_outer.transform(X_outer_test_resid)\n",
    "\n",
    "    # Train Ridge with best alpha on full outer training set (on RESIDUALIZED features)\n",
    "    model_outer = Ridge(alpha=best_alpha).fit(X_outer_train_res_scaled, y_outer_train)\n",
    "\n",
    "    # Evaluate on outer test set\n",
    "    y_outer_test_pred = model_outer.predict(X_outer_test_res_scaled)\n",
    "    R2_outer_test = r2_score(y_outer_test, y_outer_test_pred)\n",
    "    r_test = np.corrcoef(y_outer_test, y_outer_test_pred)[0, 1]\n",
    "    print(f\"  Best alpha (with confound removal): {best_alpha}\")\n",
    "    print(f\"  Outer Test R² (with confound removal): {R2_outer_test:.3f}\")\n",
    "    print(f\"  Outer Test Pearson r (with confound removal): {r_test:.3f}\")\n",
    "\n",
    "    # Evaluate train performance on outer_train (to check for overfitting)\n",
    "    y_outer_train_pred = model_outer.predict(X_outer_train_res_scaled)\n",
    "    R2_outer_train = r2_score(y_outer_train, y_outer_train_pred)\n",
    "    r_train = np.corrcoef(y_outer_train, y_outer_train_pred)[0, 1]\n",
    "    print(f\"  Outer Train R² (with confound removal): {R2_outer_train:.3f}\")\n",
    "    print(f\"  Outer Train Pearson r (with confound removal): {r_train:.3f}\")    \n",
    "\n",
    "    # Store all relevant info for this outer fold\n",
    "    outer_results_with_cnfdrm.append({\n",
    "        \"fold\": outer_fold,\n",
    "        \"best_alpha\": best_alpha,\n",
    "        \"R2_train\": R2_outer_train,\n",
    "        \"R2_test\": R2_outer_test,\n",
    "        \"r_train\": r_train,\n",
    "        \"r_test\": r_test,\n",
    "        \"model\": model_outer,\n",
    "        \"scaler\": scaler_outer,\n",
    "        \"X_train_idx\": outer_train_idx,\n",
    "        \"X_test_idx\": outer_test_idx\n",
    "    })\n",
    "\n",
    "    print(f\"----- Done with outer fold {outer_fold+1} ------\\n\")\n",
    "\n",
    "# Summary\n",
    "mean_test_r2_withcnfdrm = np.mean([res['R2_test'] for res in outer_results_with_cnfdrm])\n",
    "mean_test_r_withcnfdrm = np.mean([res['r_test'] for res in outer_results_with_cnfdrm])\n",
    "mean_train_r2_withcnfdrm = np.mean([res['R2_train'] for res in outer_results_with_cnfdrm])\n",
    "mean_train_r_withcnfdrm = np.mean([res['r_train'] for res in outer_results_with_cnfdrm])\n",
    "print(\n",
    "    f\"\\nNested CV summary over {n_outer} folds (with confound removal): \\n\"\n",
    "    f\"mean R2_test={mean_test_r2_withcnfdrm:.3f}, mean r_test={mean_test_r_withcnfdrm:.3f}, \\n\"\n",
    "    f\"mean R2_train={mean_train_r2_withcnfdrm:.3f}, mean r_train={mean_train_r_withcnfdrm:.3f}\",\n",
    "    )\n",
    "print(\"Best alphas per outer fold (with confound removal):\", best_alphas_outer_with_cnfdrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412903f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of train and test outer CV performance with and without confound removal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Extract values ---\n",
    "# Non-confound adjusted\n",
    "folds = [res['fold']+1 for res in outer_results]\n",
    "r2_train_no = [res['R2_train'] for res in outer_results]\n",
    "r2_test_no = [res['R2_test'] for res in outer_results]\n",
    "\n",
    "# Confound adjusted\n",
    "folds_cnf = [res['fold']+1 for res in outer_results_with_cnfdrm]\n",
    "r2_train_cnf = [res['R2_train'] for res in outer_results_with_cnfdrm]\n",
    "r2_test_cnf = [res['R2_test'] for res in outer_results_with_cnfdrm]\n",
    "\n",
    "# --- Plot ---\n",
    "x = np.arange(len(folds))\n",
    "width = 0.35\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# --- Left subplot: No confound removal ---\n",
    "ax = axes[0]\n",
    "bars1 = ax.bar(x - width/2, r2_train_no, width, label=\"Train R²\", color=\"skyblue\")\n",
    "bars2 = ax.bar(x + width/2, r2_test_no, width, label=\"Test R²\", color=\"salmon\")\n",
    "ax.set_title(\"No Confound Removal\")\n",
    "ax.set_xlabel(\"Outer Fold\")\n",
    "ax.set_ylabel(\"R² Score\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(folds)\n",
    "ax.legend()\n",
    "\n",
    "# Add numbers on top\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f\"{height:.2f}\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "# --- Right subplot: With confound removal ---\n",
    "ax = axes[1]\n",
    "bars1 = ax.bar(x - width/2, r2_train_cnf, width, label=\"Train R²\", color=\"skyblue\")\n",
    "bars2 = ax.bar(x + width/2, r2_test_cnf, width, label=\"Test R²\", color=\"salmon\")\n",
    "ax.set_title(\"With Confound Removal\")\n",
    "ax.set_xlabel(\"Outer Fold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(folds_cnf)\n",
    "ax.legend()\n",
    "\n",
    "# Add numbers on top\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f\"{height:.2f}\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Train vs Test R² per Outer Fold (Nested CV)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bf7d1",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "1. What do you take from this plot?\n",
    "2. Would you make any adjustments to your modeling setup, if so which and whz, if not, why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FZJ_collaborations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
